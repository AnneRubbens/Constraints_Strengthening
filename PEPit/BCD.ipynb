{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d6e52-9003-4af0-b6ce-0aa12d8b001f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install PEPit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b151fc-e49c-413c-8f80-94e742e359c2",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "This notebook exemplifies the results of [1, Section 3.3] (and in particular those of [1, Proposition 7]) for the numerical analysis of cyclic and randomized block-coordinate descent algorithms for convex optimization.\n",
    "(the improvements in terms of performance guarantees is extremely small, but exist)\n",
    "\n",
    "> [1] Anne Rubbens, Julien M. Hendrickx, Adrien Taylor. \"One-point extensions of function and operator classes.\"\n",
    "\n",
    "The code requires the installation of the [PEPit](https://pepit.readthedocs.io/en/latest/) package, e.g., by uncommenting the pip install command above.\n",
    "\n",
    "# Function class: refined characterization of convex functions that are smooth by block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db148a8-14f4-495b-80e4-a447d86c9c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PEPit import PEP\n",
    "from PEPit.function import Function\n",
    "from PEPit.block_partition import BlockPartition\n",
    "\n",
    "from PEPit.functions import BlockSmoothConvexFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c751c4c7-27f6-480f-8bb1-882c3cecc200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RefinedBlockSmoothConvexFunction(Function):\n",
    "    def __init__(self,\n",
    "                 partition,\n",
    "                 L,\n",
    "                 lam=[0.],\n",
    "                 is_leaf=True,\n",
    "                 decomposition_dict=None,\n",
    "                 reuse_gradient=True,\n",
    "                 name=None):\n",
    "        \n",
    "        super().__init__(is_leaf=is_leaf,\n",
    "                         decomposition_dict=decomposition_dict,\n",
    "                         reuse_gradient=True,\n",
    "                         name=name,\n",
    "                         )\n",
    "\n",
    "        # Store partition and L\n",
    "        assert isinstance(partition, BlockPartition)\n",
    "        if partition.get_nb_blocks() > 1:\n",
    "            assert isinstance(lam, list)\n",
    "            assert isinstance(L, list)\n",
    "            assert len(L) == partition.get_nb_blocks()\n",
    "            for Li in L:\n",
    "                assert Li < np.inf\n",
    "            for lami in lam:\n",
    "                assert lami <= 1/2\n",
    "                assert lami >= 0\n",
    "\n",
    "        self.partition = partition\n",
    "        self.lam = lam\n",
    "        self.L = L\n",
    "\n",
    "    def add_class_constraints(self):\n",
    "        \"\"\"\n",
    "        Formulates the list of necessary constraints for interpolation for self (block smooth convex function);\n",
    "        see [1, Proposition 7].\n",
    "\n",
    "        \"\"\"\n",
    "        # Set function ID\n",
    "        function_id = self.get_name()\n",
    "        if function_id is None:\n",
    "            function_id = \"Function_{}\".format(self.counter)\n",
    "\n",
    "        # Set tables_of_constraints attributes\n",
    "        for m in range(self.partition.get_nb_blocks()):\n",
    "            for l in range(self.partition.get_nb_blocks()):\n",
    "                self.tables_of_constraints[\"smoothness_convexity_block_{}{}\".format(m, l)] = [[[]]]*len(self.list_of_points)\n",
    "\n",
    "        # Browse list of points and create interpolation constraints\n",
    "        for i, point_i in enumerate(self.list_of_points):\n",
    "\n",
    "            xi, gi, fi = point_i\n",
    "            xi_id = xi.get_name()\n",
    "            if xi_id is None:\n",
    "                xi_id = \"Point_{}\".format(i)\n",
    "\n",
    "            for j, point_j in enumerate(self.list_of_points):\n",
    "\n",
    "                xj, gj, fj = point_j\n",
    "                xj_id = xj.get_name()\n",
    "                if xj_id is None:\n",
    "                    xj_id = \"Point_{}\".format(j)\n",
    "                \n",
    "                for k, point_k in enumerate(self.list_of_points):\n",
    "                    xk, gk, fk = point_k\n",
    "                    xk_id = xk.get_name()\n",
    "                    if xk_id is None:\n",
    "                        xk_id = \"Point_{}\".format(k)\n",
    "                        \n",
    "                    if not (point_i == point_j and point_i == point_k):\n",
    "                        for lam in self.lam:\n",
    "                            for m in range(self.partition.get_nb_blocks()):\n",
    "                                for l in range(self.partition.get_nb_blocks()):\n",
    "                                    # partial gradients for block m\n",
    "                                    gim = self.partition.get_block(gi, m)\n",
    "                                    gjm = self.partition.get_block(gj, m)\n",
    "                                    gkm = self.partition.get_block(gk, m)\n",
    "                                    \n",
    "                                    # partial gradients for block l\n",
    "                                    gjl = self.partition.get_block(gj, l)\n",
    "                                    gkl = self.partition.get_block(gk, l)\n",
    "                                    \n",
    "                                    # Necessary conditions for interpolation\n",
    "                                    constraint = (0 >= (1-lam) * (-fi + fj + gj * (xi - xj) + 1 / (2 * self.L[m]) * (gim - gjm) ** 2) +\n",
    "                                                  lam * (-fi + fk + gk * (xi - xk) + 1/(2 * self.L[m]) * (gim - gkm) ** 2) +\n",
    "                                                  lam * (1-lam)  * ( 1 / (2 * self.L[l]) * (gjl - gkl) ** 2 - 1 / (2 * self.L[m]) * (gjm - gkm) ** 2))\n",
    "                                    \n",
    "                                    constraint.set_name(\"IC_{}_smoothness_convexity_block_{}{}({}, {}, {})\".format(function_id, m, l, xi_id, xj_id, xk_id))\n",
    "                                    self.tables_of_constraints[\"smoothness_convexity_block_{}{}\".format(m, l)][i].append(constraint)\n",
    "                                    self.list_of_class_constraints.append(constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864926b9-d81d-41ce-8368-8f53f7b94f2e",
   "metadata": {},
   "source": [
    "## Example 1: cyclic block-coordinate descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea66a440-7c3c-4e3f-9569-718339a05c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for comparison\n",
    "import PEPit.examples.unconstrained_convex_minimization.cyclic_coordinate_descent as CCD\n",
    "\n",
    "def wc_cyclic_coordinate_descent_refined(L, n, refinment_param=[.375], wrapper=\"cvxpy\", solver=None, verbose=1):\n",
    "\n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a partition of the ambient space in d blocks of variables\n",
    "    d = len(L)\n",
    "    partition = problem.declare_block_partition(d=d)\n",
    "\n",
    "    # Declare a strongly convex smooth function\n",
    "    func = problem.declare_function(RefinedBlockSmoothConvexFunction, L=L, lam=refinment_param, partition=partition)\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_* and corresponding function value fs = f_*\n",
    "    xs = func.stationary_point()\n",
    "    fs = func(xs)\n",
    "\n",
    "    # Then define the starting point x0 of the algorithm\n",
    "    x0 = problem.set_initial_point()\n",
    "\n",
    "    # Set the initial constraint that is the distance between x0 and x^*\n",
    "    problem.set_initial_condition((x0 - xs) ** 2 <= 1)\n",
    "\n",
    "    # Run n steps of the GD method\n",
    "    x = x0\n",
    "    for k in range(n):\n",
    "        i = k % d\n",
    "        x = x - 1 / L[i] * partition.get_block(func.gradient(x), i)\n",
    "\n",
    "    # Set the performance metric to the function values accuracy\n",
    "    problem.set_performance_metric(func(x) - fs)\n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_verbose = max(verbose, 0)\n",
    "    pepit_tau = problem.solve(wrapper=wrapper, solver=solver, verbose=pepit_verbose)\n",
    "\n",
    "    # Compute theoretical guarantee (for comparison)\n",
    "    theoretical_tau = None\n",
    "\n",
    "    # Print conclusion if required\n",
    "    if verbose != -1:\n",
    "        print('*** Example file: worst-case performance of cyclic coordinate descent with fixed step-sizes ***')\n",
    "        print('\\tPEPit guarantee:\\t f(x_n)-f_* <= {:.6} ||x_0 - x_*||^2'.format(pepit_tau))\n",
    "\n",
    "    # Return the worst-case guarantee of the evaluated method (and the reference theoretical value)\n",
    "    return pepit_tau, theoretical_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "635d1aa8-7e77-4e65-abc3-60f49f60f52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Cyclic coordinate descent *** \n",
      "\tPEPit guarantee (std inequalities):\t f(x_n)-f_* <= 0.22515 ||x_0 - x_*||^2\n",
      "\tPEPit guarantee (refined inequalities):\t f(x_n)-f_* <= 0.222741 ||x_0 - x_*||^2\n",
      "\tIs the refined inequality stronger? True\n"
     ]
    }
   ],
   "source": [
    "L = [1,1]\n",
    "n = 2\n",
    "verbose = -1\n",
    "refinment_param = [.375]\n",
    "pepit_tau_refined, _ = wc_cyclic_coordinate_descent_refined( L, n, refinment_param, verbose = verbose )\n",
    "pepit_tau, _ = CCD.wc_cyclic_coordinate_descent( L, n, verbose = verbose )\n",
    "\n",
    "print('*** Cyclic coordinate descent *** ')\n",
    "print('\\tPEPit guarantee (std inequalities):\\t f(x_n)-f_* <= {:.6} ||x_0 - x_*||^2'.format(pepit_tau))\n",
    "print('\\tPEPit guarantee (refined inequalities):\\t f(x_n)-f_* <= {:.6} ||x_0 - x_*||^2'.format(pepit_tau_refined))\n",
    "print('\\tIs the refined inequality stronger? {}'.format(pepit_tau_refined<=pepit_tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec04f1a-4b94-47f2-9ffc-87427fdfa8a2",
   "metadata": {},
   "source": [
    "## Example 2: randomized block-coordinate descent\n",
    "\n",
    "Let $A_t\\geq 0$. We compute the largest possible $A_{t+1}$ such that the inequality \n",
    "\n",
    "$$ E_{i_k} \\left[ A_{t+1}  (f(x^{(i_k)}_{k+1})-f(x_\\star)) + \\tfrac12 \\|x_{k+1}^{(i_k)} -x_\\star\\|_{\\{L_i\\}}^2 \\right]\\leqslant A_t(f(x_{k})-f(x_\\star)) + \\tfrac12 \\|x_{k} -x_\\star\\|_{\\{L_i\\}}^2 $$\n",
    "\n",
    "is valid for all functions that are convex and smooth by blocks, and for all points $x_{k+1},x_k,x_\\star$ ( such that $x_\\star$ is a minimizer of $f$ and such that $x_{k+1}^{(i_k)}$ is obtained by RBCD updating the $i_k$th block (with stepsize $1/L_i$))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad4f60b-3bb2-49f5-87b1-865078fc08a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wc_randomized_coordinate_descent_smooth_convex(L, gamma, d, At, wrapper=\"mosek\", solver=None, verbose=1):\n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a partition of the ambient space in d blocks of variables\n",
    "    partition = problem.declare_block_partition(d=d)\n",
    "\n",
    "    # Declare a smooth convex function by blocks\n",
    "    func = problem.declare_function(BlockSmoothConvexFunction, L=L, partition=partition )\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_* and corresponding function value fs = f_*\n",
    "    xs = func.stationary_point()\n",
    "    fs = func(xs)\n",
    "\n",
    "    # Then define the point x_{t-1} of the algorithm\n",
    "    xt_minus_1 = problem.set_initial_point()\n",
    "\n",
    "    # Define the Lyapunov function\n",
    "    def flyap(x):\n",
    "        fl = (func(x) - fs)\n",
    "        return  fl\n",
    "    def distlyap(x):\n",
    "        distl = 0 * (func(x) - fs)\n",
    "        for i in range(d):\n",
    "            distl = distl + L[i] / 2 * (partition.get_block(x - xs,i)) ** 2\n",
    "        return distl\n",
    "\n",
    "    # Compute all the possible outcomes of the randomized coordinate descent step\n",
    "    gt_minus_1 = func.gradient(xt_minus_1)\n",
    "    xt_list = [xt_minus_1 - gamma[i] * partition.get_block(gt_minus_1, i) for i in range(d)]\n",
    "\n",
    "    # Compute the expected value of the Lyapunov from the different possible outcomes\n",
    "    func_t = np.mean([flyap(xt) for xt in xt_list])\n",
    "    dist_t = np.mean([distlyap(xt) for xt in xt_list])\n",
    "\n",
    "    # Set the initial condition\n",
    "    problem.set_initial_condition( At * flyap(xt_minus_1) + distlyap(xt_minus_1) - dist_t <= 1 )\n",
    "\n",
    "    # Set the performance metric to the variance\n",
    "    problem.set_performance_metric(func_t)\n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_verbose = max(verbose, 0)\n",
    "    pepit_tau = problem.solve(wrapper=wrapper, solver=solver, verbose=pepit_verbose)\n",
    "\n",
    "    # Return the worst-case guarantee of the evaluated method (and the reference theoretical value)\n",
    "    return 1/pepit_tau\n",
    "\n",
    "def wc_randomized_coordinate_descent_smooth_convex_refined(L, gamma, d, At, refinment_param, wrapper=\"mosek\", solver=None, verbose=1):\n",
    "    # Instantiate PEP\n",
    "    problem = PEP()\n",
    "\n",
    "    # Declare a partition of the ambient space in d blocks of variables\n",
    "    partition = problem.declare_block_partition(d=d)\n",
    "\n",
    "    # Declare a smooth convex function by blocks\n",
    "    func = problem.declare_function(RefinedBlockSmoothConvexFunction, L=L, lam=refinment_param, partition=partition )\n",
    "\n",
    "    # Start by defining its unique optimal point xs = x_* and corresponding function value fs = f_*\n",
    "    xs = func.stationary_point()\n",
    "    fs = func(xs)\n",
    "\n",
    "    # Then define the point x_{t-1} of the algorithm\n",
    "    xt_minus_1 = problem.set_initial_point()\n",
    "\n",
    "    # Define the Lyapunov function\n",
    "    def flyap(x):\n",
    "        fl = (func(x) - fs)\n",
    "        return  fl\n",
    "    def distlyap(x):\n",
    "        distl = 0 * (func(x) - fs)\n",
    "        for i in range(d):\n",
    "            distl = distl + L[i] / 2 * (partition.get_block(x - xs,i)) ** 2\n",
    "        return distl\n",
    "\n",
    "    # Compute all the possible outcomes of the randomized coordinate descent step\n",
    "    gt_minus_1 = func.gradient(xt_minus_1)\n",
    "    xt_list = [xt_minus_1 - gamma[i] * partition.get_block(gt_minus_1, i) for i in range(d)]\n",
    "\n",
    "    # Compute the expected value of the Lyapunov from the different possible outcomes\n",
    "    func_t = np.mean([flyap(xt) for xt in xt_list])\n",
    "    dist_t = np.mean([distlyap(xt) for xt in xt_list])\n",
    "\n",
    "    # Set the initial condition\n",
    "    problem.set_initial_condition( At * flyap(xt_minus_1) + distlyap(xt_minus_1) - dist_t <= 1 )\n",
    "\n",
    "    # Set the performance metric to the variance\n",
    "    problem.set_performance_metric(func_t)\n",
    "\n",
    "    # Solve the PEP\n",
    "    pepit_verbose = max(verbose, 0)\n",
    "    pepit_tau = problem.solve(wrapper=wrapper, solver=solver, verbose=pepit_verbose)\n",
    "\n",
    "    # Return the worst-case guarantee of the evaluated method (and the reference theoretical value)\n",
    "    return 1/pepit_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8d4bae-b529-490a-be22-5952dd46b599",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Cyclic coordinate descent *** \n",
      "\tPEPit guarantee (std inequalities):\t A_(t+1)=10.50076493496037\n",
      "\tPEPit guarantee (refined inequalities):\t A_(t+1)=10.502234560108477\n",
      "\tIs the refined inequality stronger (larger A_(t+1))? True\n",
      "\tNote: the difference between the bounds is typically very small and might disappear as the problem becomes larger (e.g., larger d) due to numerical inaccuracies.\n"
     ]
    }
   ],
   "source": [
    "L = [2.,.5]\n",
    "d = len(L)\n",
    "gamma=1./np.array(L)\n",
    "At = 10\n",
    "verbose = -1\n",
    "refinment_param = [.375]\n",
    "At1_refined = wc_randomized_coordinate_descent_smooth_convex_refined(L, gamma, d, At, refinment_param, verbose=verbose)\n",
    "At1 = wc_randomized_coordinate_descent_smooth_convex(L, gamma, d, At, verbose=verbose)\n",
    "\n",
    "print('*** Cyclic coordinate descent *** ')\n",
    "print('\\tPEPit guarantee (std inequalities):\\t A_(t+1)={}'.format(At1))\n",
    "print('\\tPEPit guarantee (refined inequalities):\\t A_(t+1)={}'.format(At1_refined))\n",
    "print('\\tIs the refined inequality stronger (larger A_(t+1))? {}'.format(At1_refined>=At1))\n",
    "print('\\tNote: the difference between the bounds is typically very small and might disappear as the problem becomes larger (e.g., larger d) due to numerical inaccuracies.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bec108-b40f-49ef-9d81-2da67e8b08b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
